#!/usr/bin/env ruby

# We Cannot use "@ .. @" for libdir, bindir, and dataroodir
# as they will appear as bash "${exec_prefix}/lib"
# So for now we will rely on them having the default value,
#  (https://www.gnu.org/software/automake/manual/html_node/Standard-Directory-Variables.html)
# Will do some gsub + eval if required latter
EXEC_PREFIX = '@prefix@'
BINDIR = File.join(EXEC_PREFIX, 'bin')
LIBDIR = File.join(EXEC_PREFIX, 'lib')
PKGLIBDIR = File.join(LIBDIR, '@PACKAGE@')
PREFIX = '@prefix@'
DATAROOTDIR = File.join(PREFIX, 'share')
DATADIR = DATAROOTDIR

LTTNG_ARCHIVE_SIZE = '50M'
LTTNG_ARCHIVE_TIMER = '60s'
LTTNG_DIRWATCH_SIZE = '500' # In MiB
LTTNG_DIRWATCH_LOCK_RETRY_DELAY = 0.1

$LOAD_PATH.unshift(DATADIR) if File.directory?(DATADIR)
require 'open3'
require 'fileutils'
require 'etc'
require 'optparse_thapi'
require 'pty'
require 'digest/md5'
require 'socket'
require 'logger'
require 'set'
require 'securerandom'
require 'yaml'
require 'date'

module LoggerRefinement
  refine(Logger) do
    def info_block(message, &block)
      info("#{message}: entry") if message
      block.call
      info("#{message}: exit") if message
    end
  end
end
using LoggerRefinement

def exec(cmd, opts: {}, debug: true)
  return Open3.capture3(opts, cmd).first unless debug

  LOGGER.info { cmd }
  LOGGER.debug { opts } unless opts.empty?

  stdout_str, stderr_str, status = Open3.capture3(opts, cmd)
  raise "#{cmd} failed" unless status.success?

  LOGGER.warn { stderr_str.strip } unless stderr_str.empty?
  LOGGER.debug { stdout_str.strip } unless stdout_str.empty?
  stdout_str
end

#    _
#   |_ |\ | \  /
#   |_ | \|  \/
#

# Cannot use refinement on `ENV`
#   as I don't know the `type` of ENV
# /!\ bad caching was changing the value of `mpi_env?, so no more caching
def env_fetch_first(*args, default: nil)
  ENV.values_at(*args).compact.first || default
end

#
#   \    / |_  o  _ |_  | o |_
#    \/\/  | | | (_ | | | | |_)
#
def ldconfig
  @ldconfig ||= begin
    ld_library_path = env_fetch_first('LD_LIBRARY_PATH', default: '').gsub(':', ' ')
    stdout_str = exec("ldconfig -vNX #{ld_library_path}",
                      opts: { 'PATH' => "#{env_fetch_first('PATH')}:/sbin" },
                      debug: false)
    stdout_str.split
  end
end

def whichlibs(pattern)
  ldconfig.reduce(['', []]) do |(header, path), l|
    # /opt/cray/libfabric/1.15.2.0/lib64:
    next [l.chop, path] if l.end_with?(':')

    # libfabric.so.1 -> libfabric.so.1.18.2
    lib = l.split(' -> ').first
    path << File.join(header, lib) if lib.match?(pattern)
    next [header, path]
  end.last
end

def whichlib64(pattern)
  # Use lazy to avoid `objdump` all the possible libs
  whichlibs(pattern).lazy.filter do |lib|
    stdout_str = exec("objdump -f #{lib}")
    # architecture: i386:x86-64, flags 0x00000150"
    stdout_str.match?(/architecture: \S+64/)
  end.first
end

#         _ ___    _
#   |\/| |_) |    |_ ._
#   |  | |  _|_   |_ | | \/
#
def in_mpi_env?
  !env_fetch_first('THAPI_JOBID', 'PALS_RANKID', 'PMI_RANK', 'OMPI_COMM_WORLD_RANK').nil?
end

def env_fetch_first_mpi(*args, default: nil)
  v = env_fetch_first(*args)
  if in_mpi_env?
    return v unless v.nil?

    LOGGER.fatal { 'A MPI JOBID is set but some sone MPI env are missing' }
    exit(1)

  else
    LOGGER.warn { "A MPI JOBIB haven't be set, but some args have been set, ignoring them" } unless v.nil?
    default
  end
end

# Multiple iprof may run in parallel (for example using ctest),
# so use a random hex by default
def mpi_job_id
  @mpi_job_id ||= env_fetch_first_mpi('THAPI_JOBID', 'PALS_APID', 'PMI_JOBID', 'OMPI_MCA_ess_base_jobid',
                                      default: SecureRandom.hex)
end

def mpi_rank_id
  env_fetch_first_mpi('PALS_RANKID', 'PMI_RANK', 'OMPI_COMM_WORLD_RANK', default: 0).to_i
end

def mpi_local_size
  env_fetch_first_mpi('PALS_LOCAL_SIZE', 'PMI_LOCAL_SIZE', 'OMPI_COMM_WORLD_LOCAL_SIZE', 'MPI_LOCALNRANKS',
                      default: 1).to_i
end

def mpi_local_rank_id
  env_fetch_first_mpi('PALS_LOCAL_RANKID', 'PMI_LOCAL_RANK', 'OMPI_COMM_WORLD_LOCAL_RANK', 'MPI_LOCALRANKID',
                      default: 0).to_i
end

def mpi_local_master?
  mpi_local_rank_id.zero?
end

def mpi_master?
  mpi_rank_id.zero?
end

#    _                    _
#   |_ _  |  _|  _  ._   |_) _. _|_ |_
#   | (_) | (_| (/_ |    |  (_|  |_ | |
#

# Prefix of the thapi_trace_dir
def prefix_processed_trace
  if OPTIONS.include?(:trace) || !OPTIONS[:analysis]
    ''
  elsif OPTIONS.include?(:timeline)
    '_interval'
  else
    '_aggreg'
  end
end

# THAPI/xprof use multiple folder
# - lttng_trace_dir_tmp
#     where the 'raw' lttng trace is saved.
#     For performance we store in a `tmp` folder.
#     When "on the fly" analys will be possible, this should not potential OOM
# - thapi_trace_dir_tmp
#     Where the "pre-processed / analysed / condensed" trace a saved.
#     Each nodes need to agree on a root folder to write into.
#  - thapi_trace_dir_root
#     Final directory exposed to the user. The `master rank`  rename the `thapi_trace_dir_tmp`
#     with a more "friendly" name. Not required if `analysis-output` is set
#
#  - lttng_home_dir
#     Home of the lttng daemon

def lttng_trace_dir_tmp
  File.join('/', 'tmp', "thapi--#{mpi_job_id}", Socket.gethostname)
end

def thapi_trace_home
  File.join(env_fetch_first('THAPI_HOME', 'HOME'), 'thapi-traces')
end

def thapi_trace_dir_tmp
  return File.join(OPTIONS[:'trace-output'], Socket.gethostname) if OPTIONS[:'trace-output']

  File.join(thapi_trace_home, "thapi#{prefix_processed_trace}--#{mpi_job_id}",
            Socket.gethostname)
end

def lttng_home_dir
  File.join('/', 'tmp', "lttng_home--#{mpi_job_id}")
end

def thapi_trace_dir_root
  raise unless mpi_master?

  return OPTIONS[:'trace-output'] if OPTIONS[:'trace-output']

  @thapi_trace_dir_root ||= begin
    # Multiple thapi can run concurrently.
    #   To avoid any race-conditions, we try to MKDIR until we succeed

    # This is solving a consensus for the name,
    # make it simpler and only allow "rank" per job to do it
    # Use ISO8601 extended format
    date = DateTime.now.iso8601

    (0..).each do |i|
      prefix = i == 0 ? '' : "_#{i}"
      thapi_uuid = date + prefix
      path = File.join(thapi_trace_home, "thapi#{prefix_processed_trace}--#{thapi_uuid}")
      begin
        Dir.mkdir(path)
      rescue SystemCallError
        next
      else
        break path
      end
    end
  end
end

#    _
#   |_)  _. ._ ._ o  _  ._
#   |_) (_| |  |  | (/_ |
#
class Sync_daemon
  SIGRTMIN = 34
  RT_SIGNAL_READY = SIGRTMIN
  RT_SIGNAL_GLOBAL_BARRIER = SIGRTMIN + 1
  RT_SIGNAL_LOCAL_BARRIER = SIGRTMIN + 2
  RT_SIGNAL_FINISH = SIGRTMIN + 3

  def _lazy_exec(&block)
    return unless in_mpi_env?

    Signal.trap(RT_SIGNAL_READY) do
      return
    end
    block.call
    sleep
  end

  def lazy_exec(message = nil, &block)
    LOGGER.info_block(message) do
      # Don't inline, it Trap doesn't work with logger
      # https://bugs.ruby-lang.org/issues/7917
      _lazy_exec(&block)
    end
  end

  def initialize
    daemon_type = env_fetch_first('THAPI_SYNC_DAEMON')
    daemon = case daemon_type
             when nil
               # Default is MPI
               if File.exist?("#{__dir__}/sync_daemon_mpi")
                 "#{__dir__}/sync_daemon_mpi"
               else
                 LOGGER.warn("No #{__dir__}/sync_daemon_mpi binary. Fall back to #{__dir__}/syc_daemon_f'")
                 "#{__dir__}/sync_daemon_fs"
               end
             when 'mpi'
               raise("No #{__dir__}/sync_daemon_mpi binary") unless File.exist?("#{__dir__}/sync_daemon_mpi")

               "#{__dir__}/sync_daemon_mpi"
             when 'fs'
               "#{__dir__}/sync_daemon_fs"
             else
               raise("Error: THAPI_SYNC_DAEMON value (#{daemon_type}) if not supported. Allowed: [mpi,fs] ")
             end

    LOGGER.debug { "spawn(#{daemon} #{Process.pid})" }
    lazy_exec("Initialize Sync_daemon #{daemon_type}") do
      @pid = spawn("#{daemon} #{Process.pid}")
    end
  end

  def finalize
    lazy_exec('Finalize Sync_daemon') do
      `kill -#{RT_SIGNAL_FINISH} #{@pid}`
    end
  end

  def local_barrier(name)
    lazy_exec("Local_barrier #{name}") do
      `kill -#{RT_SIGNAL_LOCAL_BARRIER} #{@pid}`
    end
  end

  def global_barrier
    lazy_exec('Global_barrier') do
      `kill -#{RT_SIGNAL_GLOBAL_BARRIER} #{@pid}`
    end
  end

  # Context manager, ensure that when the block yield is exited
  # we always call clean-up the daemon
  def self.open
    yield f = new
  rescue StandardError
    raise
  ensure
    return unless f
    f.global_barrier
    f.finalize
  end
end

#
#   | _|_ _|_ ._   _
#   |_ |_  |_ | | (_|
#                  _|
def lttng_session_uuid
  Digest::MD5.hexdigest(lttng_trace_dir_tmp)
end

def sampling?
  return false unless OPTIONS[:sample]

  env_fetch_first('LTTNG_UST_SAMPLING_MASTER_ONLY', default: '0') == '0' || mpi_local_master?
end

def env_tracers
  # Return the list of backends (used by local master to enable lttng events)
  # and the ENV used by any traced-ranks to preload THAPI tracers
  need_backend = mpi_local_master?
  need_env = Set[-1, mpi_rank_id].intersect?(OPTIONS[:'traced-ranks'])

  # Early exit to be nice with the FileSystem
  return [[], {}] unless need_backend || need_env

  h = Hash.new { |h, k| h[k] = [] }
  backends = []

  [%w[opencl cl libOpenCL libTracerOpenCL],
   %w[ze ze libze_loader libTracerZE],
   %w[cuda cuda libcuda libTracerCUDA],
   %w[hip hip libamdhip64 libTracerHIP],
   %w[mpi mpi libmpi libTracerMPI],
  ].each do |name, bt_name, lib, libtracer|
    # Backend requested, skip omp. It will be handled in a custom case bellow
    next unless OPTIONS[:'backend-names'].include?(bt_name)

    # Find and Save the original lib path
    libenv = "LTTNG_UST_#{name.upcase}_#{lib.upcase}"
    if (e = env_fetch_first(libenv))
      LOGGER.warn("#{libenv} was already set, will use this path #{e} for the #{name} loader")
      # TODO: Verify that this guy exist?
    elsif (libpath = whichlib64("#{lib}.so"))
      h[libenv] = libpath
    else
      LOGGER.warn("No #{lib}.so found in LD_LIBRARY_PATH")
      next
    end
    backends << bt_name
    # Add our "stud" library to the path
    h[%w[LD_LIBRARY_PATH prepend]] << File.join(PKGLIBDIR, name)
    # Preload our own lib
    h[%w[LD_PRELOAD prepend]] << File.join(LIBDIR, "#{libtracer}.so")
    h["LTTNG_UST_#{name.upcase}_PROFILE"] = 1 if OPTIONS[:profile]
    h["LTTNG_UST_#{name.upcase}_VERBOSE"] = 1 if LOGGER.level <= Logger::DEBUG
  end
  # Usr Apps need to be launched with this to support blocking.
  h['LTTNG_UST_ALLOW_BLOCKING'] = 1
  # Customization
  h['LTTNG_UST_ZE_PARANOID_DRIFT'] = 1 if OPTIONS[:'backend-names'].include?('ze') && OPTIONS[:profile]
  if OPTIONS[:'backend-names'].include?('omp')
    backends << 'omp'
    h['OMP_TOOL_LIBRARIES'] = File.join(LIBDIR, 'libTracerOMPT.so')
  end

  # Sample
  if sampling?
    LOGGER.debug('Sampling Enabled')
    h['LTTNG_UST_SAMPLING'] = 1
    h['LTTNG_UST_SAMPLING_ENERGY'] = 1
    h['ZES_ENABLE_SYSMAN'] = 1 if OPTIONS[:'backend-names'].include?('ze')
  end

  backends = [] unless need_backend
  h = {} unless need_env
  LOGGER.info("Backends found: #{backends}")
  LOGGER.debug("User app env: #{h}")

  [backends, h.freeze]
end

def launch_usr_bin(env, cmd)
  LOGGER.info { "Launch_usr_bin #{cmd}" }
  # Transform list to bash env
  #   prepending to current env if already existing
  #   we don't modify `ENV` direclly to avoid by-construction any side-effect
  bash_env = env.filter_map do |k0, v|
    # Add default mode to k0
    k, mode = ([k0] + ['ignore']).flatten[0..1]
    vs = [v].flatten
    case mode
    when 'ignore'
      if env_fetch_first(k)
        LOGGER.warn { "#{k} already existed in ENV. We will not overwrite it" }
        next
      end
    when 'prepend'
      if (cv = env_fetch_first(k))
        LOGGER.warn { "#{k} already existed in ENV. The value #{v} will be prepended to it" }
        vs.append(cv)
      end
    end

    [k, vs.join(':')]
  end.to_h

  begin
    PTY.spawn(bash_env, *cmd) do |stdout, _stdin, _pid|
      stdout.each { |line| print line }
    rescue Errno::EIO
    end
  # Not sure how this exception can be triggered
  rescue PTY::ChildExited
    LOGGER.warn { 'Application Exited' }
  rescue Interrupt
    LOGGER.warn { 'Application Received Interrupt Signal' }
  end
end

def enable_events_ze(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  case tracing_mode
  when 'minimal'
    exec("#{lttng_enable} $(cat #{datadir}/babeltrace_zeprofiling_apis.txt)")
    exec("#{lttng_enable} lttng_ust_ze_properties:device_timer")
  when 'full'
    exec("#{lttng_enable} lttng_ust_ze_build:log*")
    exec("#{lttng_enable} lttng_ust_ze_profiling:*") if profiling
    exec("#{lttng_enable} lttng_ust_ze_properties:*")
    exec("#{lttng_enable} lttng_ust_ze:*")
    exec("#{lttng_enable} lttng_ust_zes:*")
    exec("#{lttng_enable} lttng_ust_zet:*")
    exec("#{lttng_enable} lttng_ust_zex:*")
  when 'default'
    # Wildcard using the * character are supported at the end of tracepoint names.
    #   https://lttng.org/man/1/lttng-enable-event/v2.8/#doc-_understanding_event_rule_conditions
    # Disable-event doesn't have wildcards
    # So we enable and disable on the same line
    exec("#{lttng_enable} lttng_ust_ze_build:log*")
    exec("#{lttng_enable} lttng_ust_ze_profiling:*") if profiling
    exec("#{lttng_enable} lttng_ust_zex:*")
    exec("#{lttng_enable} lttng_ust_ze_properties:*")
    ze_disable_events = ['lttng_ust_ze:zeKernelSetArgumentValue*', 'lttng_ust_ze:ze*Get*Properties*',
                         'lttng_ust_ze:zeKernelGetName*', 'lttng_ust_ze:zeKernelSetGlobalOffsetExp*',
                         'lttng_ust_ze:zeKernelSuggestGroupSize*']
    ze_disable_query = ['lttng_ust_ze:*QueryStatus*', 'lttng_ust_ze:*ProcAddrTable*']
    ze_disable_loader = ['lttng_ust_ze:*Loader*']
    ze_disable = ze_disable_events + ze_disable_query + ze_disable_loader
    exec("#{lttng_enable} lttng_ust_ze:* -x #{ze_disable.join(',')}")
  else
    raise("Tracing mode #{tracing_mode} not supported")
  end
end

def enable_events_cl(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  case tracing_mode
  when 'full'
    exec("#{lttng_enable} lttng_ust_opencl:*")
    exec("#{lttng_enable} lttng_ust_opencl_profiling:*") if profiling
    exec("#{lttng_enable} lttng_ust_opencl_devices:*")
    exec("#{lttng_enable} lttng_ust_opencl_arguments:*")
    exec("#{lttng_enable} lttng_ust_opencl_build:infos*")
  when 'default'
    exec("#{lttng_enable} lttng_ust_opencl_profiling:*") if profiling
    exec("#{lttng_enable} lttng_ust_opencl_devices:*")
    exec("#{lttng_enable} lttng_ust_opencl_arguments:*")
    exec("#{lttng_enable} lttng_ust_opencl_build:infos*")
    # Wildcard using the * character are supported at the end of tracepoint names.
    #   https://lttng.org/man/1/lttng-enable-event/v2.8/#doc-_understanding_event_rule_conditions
    # Disable-event doesn't have wildcards
    # So we enable and disable on the same line
    opencl_disable = ['lttng_ust_opencl:clSetKernelArg*', 'lttng_ust_opencl:clGetKernelArg*',
                      'lttng_ust_opencl:clSetKernelExecInfo*', 'lttng_ust_opencl:clGetKernelInfo*',
                      'lttng_ust_opencl:clGetMemAllocInfoINTEL*']

    exec("#{lttng_enable} lttng_ust_opencl:* -x #{opencl_disable.join(',')}")
  when 'minimal'
    LOGGER.debug("Tracing mode #{tracing_mode} not supported for OpenCL")
  else
    raise("Tracing mode #{tracing_mode} not supported")
  end
end

def enable_events_cuda(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  exec("#{lttng_enable} lttng_ust_cuda:*")
  exec("#{lttng_enable} lttng_ust_cuda_properties")
  exec("#{lttng_enable} lttng_ust_cuda_profiling:*") if profiling
end

def enable_events_hip(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  exec("#{lttng_enable} lttng_ust_hip:*")
end

def enable_events_omp(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  exec("#{lttng_enable} lttng_ust_ompt:*target*")
end

def enable_events_mpi(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  exec("#{lttng_enable} lttng_ust_mpi:*")
  exec("#{lttng_enable} lttng_ust_mpi_type:*")
end

def enable_events_metadata(channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  exec("#{lttng_enable} lttng_ust_thapi:*")
end

def lm_setup_lttng(backends)
  raise unless mpi_local_master?

  # Spawning the sessiond Daemon will crash
  #   if LTTNG_HOME doesn't exist
  FileUtils.mkdir_p(lttng_home_dir)
  FileUtils.mkdir_p(lttng_trace_dir_tmp)
  exec('lttng-sessiond --daemonize')
  exec("lttng create #{lttng_session_uuid} -o #{lttng_trace_dir_tmp}")

  File.write(File.join(lttng_trace_dir_tmp, 'thapi_metadata.yaml'), { type: 'lttng' }.to_yaml)

  channel_name = 'blocking-channel'
  exec("lttng enable-channel --userspace --session=#{lttng_session_uuid} --blocking-timeout=inf #{channel_name}")
  exec("lttng add-context    --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} -t vpid -t vtid")

  # Enable backend events
  (backends + ['metadata']).each do |name|
    send("enable_events_#{name}", channel_name,
         tracing_mode: OPTIONS[:'tracing-mode'],
         profiling: OPTIONS[:profile])
  end

  if OPTIONS.include?(:sample)
    channel_name = 'non-blocking-channel'
    exec("lttng enable-channel --userspace --session=#{lttng_session_uuid} #{channel_name}")
    exec("lttng add-context    --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} -t vpid -t vtid")
    exec("lttng enable-event   --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} lttng_ust_sampling:*")

    if OPTIONS[:'backend-names'].include?('ze')
      exec("lttng enable-event  --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} lttng_ust_ze_sampling:*")
    end

  end

  if OPTIONS[:archive]
    exec("lttng enable-rotation --session=#{lttng_session_uuid} --size=#{LTTNG_ARCHIVE_SIZE} --timer=#{LTTNG_ARCHIVE_TIMER}")
  end
  exec("lttng start #{lttng_session_uuid}")
end

def lm_lttng_teardown_session
  raise unless mpi_local_master?
  exec("lttng rotate #{lttng_session_uuid}") if OPTIONS[:archive]
  exec("lttng destroy #{lttng_session_uuid}")
end

def lm_lttng_kill_sessiond
  raise unless mpi_local_master?
  # Need to kill the sessiond Daemon. It's safe because each job has their own
  #
  # In theory, opening the lttng-sessiond.pid file is racy.
  # It's possible that the sessiond spawned and didn't yet write the file
  # In practice, there is a lot of work between the spawn and the write, so it should be ok
  LOGGER.info_block('Killing sessiond') do
    pid = File.read(File.join(lttng_home_dir, '.lttng', 'lttng-sessiond.pid')).to_i
    Process.kill('SIGKILL', pid)
    FileUtils.rm_f(lttng_home_dir)
  end
end

#    _                                   _
#   |_)  _. |_   _  | _|_ ._ _.  _  _     )
#   |_) (_| |_) (/_ |  |_ | (_| (_ (/_   /_
#
def lm_babeltrace(backends)
  raise unless mpi_local_master?
  # No need to run babeltrace_thapi
  return if OPTIONS.include?(:trace) || !OPTIONS[:analysis]

  type = OPTIONS.include?(:timeline) ? 'interval' : 'aggreg'
  opts = ["to_#{type}"]
  opts << "--output #{thapi_trace_dir_tmp}"
  opts << "--backends #{backends.join(',')}"
  opts << '--no-discard-metadata' if type == 'aggreg' && OPTIONS.include?(:'kernel-verbose')

  if OPTIONS[:archive]
    read_file = File.join(lttng_trace_dir_tmp, 'bt_archive_ready')
    opts << "--archive #{lttng_session_uuid} --archive-session-found-file-path=#{read_file}"
    cmd = "#{BINDIR}/babeltrace_thapi #{opts.join(' ')} -- #{lttng_trace_dir_tmp}"
    LOGGER.debug(cmd)
    pid_bt = spawn(cmd)

    cmd = "dirwatch.py --log-level=CRITICAL #{lttng_session_uuid} #{LTTNG_DIRWATCH_SIZE}"
    LOGGER.debug(cmd)
    pid_dirwatch = spawn(cmd)

    sleep(LTTNG_DIRWATCH_LOCK_RETRY_DELAY) until File.exist?(read_file)
    [pid_bt, pid_dirwatch]
  else
    exec("#{BINDIR}/babeltrace_thapi #{opts.join(' ')} -- #{lttng_trace_dir_tmp}")
  end
end

#   _
#  |_) ._ _   _  _   _  _ o ._   _
#  |   | (_) (_ (/_ _> _> | | | (_|
#                                _|

# Some naming convention 
# lm == function executed only local_master
# gm ==  function executed only global_master

def lm_move_to_shared
  raise unless mpi_local_master?
  if OPTIONS.include?(:trace) || !OPTIONS[:analysis]
    # The Apps finished, lttng finished, need to move to the shared tmp folder
    FileUtils.mkdir_p(File.dirname(thapi_trace_dir_tmp))
    # NOTE: I don't understand `mv`
    #       File.mv(a, b) will put a into b (aka a/b)
    #       FileUtils.rename(a,b) will move a as b, but may
    #         raise Invalid cross-device error.
    #       So we use `exec(mv -T a b)`, this have the added benefice of logging
    exec("mv #{lttng_trace_dir_tmp} #{thapi_trace_dir_tmp}")
  else
    # `lm_babeltrace` finished, can remove `tmp` folder
    FileUtils.rm_f(lttng_trace_dir_tmp)
  end
end

def gm_rename_folder
  raise unless mpi_master?
  # All process have put their file into `thapi_trace_dir_tmp/hostname`.
  # `thapi_trace_dir_tmp` is using the MPI_JOB_ID
  # Replace it with a better name, and update the root metadata.

  thapi_trace_dir_tmp_root = File.dirname(thapi_trace_dir_tmp)
  # Because of `traced-rank`, `mpi_master` may not have any trace available,
  # so find the first hostname who have a metadata
  FileUtils.cp(Dir.glob("#{thapi_trace_dir_tmp_root}/*/thapi_metadata.yaml").first,
               File.join(thapi_trace_dir_tmp_root, 'thapi_metadata.yaml'))

  # NOTE: I don't understand `mv`
  #       File.mv(a, b) will put a into b (aka a/b)
  #       FileUtils.rename(a,b) will move a as b, but may
  #         raise Invalid cross-device error.
  #       So we use `exec(mv -T a b)`, this have the added benefice of logging
  exec("mv -T #{thapi_trace_dir_tmp_root} #{thapi_trace_dir_root}") unless OPTIONS[:'trace-output']
  thapi_trace_dir_root
end

# Start, Stop lttng, amd do the on-node analsysis
def trace_and_on_node_processing(usr_argv)
  # Global barrier at exit
  Sync_daemon.open do |syncd|
    # Load Tracers and APILoaders Lib
    backends, h = env_tracers

    # All ranks need to set the LLTTNG_HOME env
    # so they can have access to the daemon
    ENV['LTTNG_HOME'] = lttng_home_dir
    LOGGER.debug("LTTNG_HOME = #{ENV.fetch('LTTNG_HOME', nil)}")

    # Only local master spawn daemons (lttng, and babeltrace)
    #  and the start the lttng-session
    pids = if mpi_local_master?
       lm_setup_lttng(backends)
       lm_babeltrace(backends) if OPTIONS[:archive]
    end
    # Other local node cannot start before lttng and the daemon
    syncd.local_barrier('waiting_for_lttng_setup')
    # Launch User Command
    launch_usr_bin(h, usr_argv)
    # We need to ensure that all the local ranks have finished
    # running the user application
    # before the local master stops the lttng session
    syncd.local_barrier('waiting_for_application_ending')

    # Everything from now on, is some local-master processing
    # The `Sync_daemon` context will handle the call to the global barrier
    # for the early exiting ranks
    return unless mpi_local_master?

    # Stop Lttng session and babeltrace daemons
    lm_lttng_teardown_session
    if OPTIONS[:archive]
      LOGGER.debug("Waiting for babeltrace_thapi and dirwatch (#{pids}) to finish")
      pids.each { |pid| Process.wait(pid) }
    end
    # we can kill the session daemon
    lm_lttng_kill_sessiond
    # Preprocess trace
    lm_babeltrace(backends) unless OPTIONS[:archive]
    lm_move_to_shared
  end
  # Global master rename the unique trace folder to a more
  # human friendly name
  gm_rename_folder if mpi_master?
end

def gm_processing(folder)
  raise unless mpi_master?

  LOGGER.info { "Postprocess #{folder}" }

  cmdname = "#{BINDIR}/babeltrace_thapi"
  backends = OPTIONS[:backends].join(',')

  args = []
  if OPTIONS.include?(:trace)
    args += ['trace', '--restrict', '--context', '--backends', backends]
  elsif OPTIONS.include?(:timeline)
    args += ['timeline', '--backends', backends, '--output-path', OPTIONS[:timeline]]
  else
    args += ['tally']
    args << '--display_kernel_verbose' if OPTIONS.include?(:'kernel-verbose')
    args << '--display_metadata' if OPTIONS.include?(:metadata)
    args << '--display_name_max_size' << OPTIONS[:'max-name-size'].to_s
    args << '--display_mode' << 'json' if OPTIONS.include?(:json)
    args << '--backends' << backends
    args << '--display' << 'extended' if OPTIONS.include?(:extended)
  end

  args += ['--', folder]

  LOGGER.debug(cmdname)
  LOGGER.debug(args)

  LOGGER.info_block('IO dump') do
    fo = if OPTIONS[:'analysis-output']
           File.open(OPTIONS[:'analysis-output'], 'w')
         else
           $stdout.dup
         end

    IO.popen([cmdname, *args]) do |pipe|
      fo.puts(pipe.readlines)
    end

    fo.close
  end
  exit(1) unless $?.success?
end

#
#    _                       _    ___
#   |_) _. ._ _ o ._   _    /  |   |
#   |  (_| | _> | | | (_|   \_ |_ _|_
#                      _|
def last_trace_saved
  Dir[File.join(thapi_trace_home, 'thapi*')].max_by { |f| File.mtime(f) }
end

# Avoid load problem
if __FILE__ == $PROGRAM_NAME
  parser = OptionParserWithDefaultAndValidation.new

  parser.banner = "Usage: #{File.basename($PROGRAM_NAME)} [options] [--] [command]"

  parser.on('--trace-output PATH', 'Define where the ctf trace will be saved',
            'Default path is something like: $THAPI_HOME/thapi-traces/thapi--%Y-%m-%d--%Hh%Mm%Ss',
            '$THAPI_HOME defaults to $HOME') do |p|
    raise(OptionParser::ParseError, "#{p} already exists") if mpi_master? && File.exist?(p)

    p
  end

  parser.on('--analysis-output PATH',
            'Define where the output of the analysis (summary, pretty printing,...) will be saved')
  # Tracing
  parser.on('-m', '--tracing-mode MODE', 'Define the category of events traced', default: 'default',
                                                                                 allowed: %w[minimal default full])
  parser.on('--traced-ranks RANK', Array, 'Select with MPI rank will be traced.',
            'Use -1 to mean all ranks.',
            default: Set[-1]) do |ranks|
    ranks.to_set do |r|
      if r.match?(/^\d+$/) || r == '-1'
        r.to_i
      else
        raise(OptionParser::ParseError,
              "Invalid value (#{r}). Only positive integers and -1 accepted")
      end
    end
  end
  parser.on('--[no-]profile', 'Trigger for device activities profiling', default: true)
  parser.on('--[no-]analysis', 'Trigger for analysis', default: true)

  # General Options
  parser.on('-b', '--backends BACKENDS', Array, "Select which and how backends' need to handled.",
            'Format: backend_name[:backend_level],...',
            default: ['mpi:3', 'omp:2', 'cl:1', 'ze:1', 'cuda:1', 'hip:1'])
  parser.on('--[no-]archive', 'Trigger for ardhive support', default: false)

  # Analysis
  parser.on('-r', '--replay [PATH]', 'Replay traces for post-mortem analysis.',
            'If PATH is omitted, it will be set to the newest trace in $HOME/thapi-traces/')
  parser.on('-t', '--trace', 'Pretty print the trace')
  parser.on('-l', '--timeline [PATH]', 'Dump the timeline of the trace to a binary file',
            'If PATH is omitted it will be set to `out.pftrace`',
            'Open trace with perfetto: https://ui.perfetto.dev/#!/viewer') do |p|
              p || 'out.pftrace'
            end
  ## Tally Specific Options
  parser.on('-j', '--json', 'The tally will be dumped as json')
  parser.on('-e', '--extended', 'The tally will be printed for each Hostname / Process / Thread / Device')
  parser.on('-k', '--kernel-verbose',
            'The tally will report kernels execution time with SIMD width and global/local sizes')
  parser.on('--max-name-size SIZE',
            OptionParser::DecimalInteger,
            'Maximum size allowed for kernels names.',
            'Use -1 to mean no limit.', default: 80)

  parser.on('-s', '--sample', 'Enabling counters sampling')

  parser.on('--metadata', 'Display the trace metadata')
  parser.on('-v', '--version', 'Print the Version String') do
    puts File.read(File.join(DATADIR, 'version'))
    exit
  end
  parser.on('-h', '--help', 'Display this message') { print_help_and_exit(parser, exit_code: 0) }

  parser.on('--debug [LEVEL]', OptionParser::DecimalInteger, 'Set the Level of debug',
            "If LEVEL is omitted the debug level with be set to #{Logger::INFO}", default: Logger::FATAL) do |d|
              d || Logger::INFO
            end

  def print_help_and_exit(parser, exit_code: 1)
    puts(parser.help)
    puts(<<~EOF
                                                            __
      For complaints, praises, or bug reports please use: <(o )___
         https://github.com/argonne-lcf/THAPI              ( ._> /
         or send email to {apl,bvideau}@anl.gov             `---'
    EOF
        )
    exit(exit_code)
  end

  # Parsing ARGV
  print_help_and_exit(parser) if ARGV.empty?

  options = {}
  begin
    parser.parse!(into: options)
  rescue OptionParser::InvalidOption => e
    puts("ERROR: #{e}. Maybe missing --?")
    print_help_and_exit(parser)
  rescue StandardError => e
    puts("ERROR: #{e}")
    print_help_and_exit(parser)
  end

  options[:'backend-names'] = options[:backends].map { |name_level| name_level.split(':').first }
  OPTIONS = options.freeze

  # Setup Logger
  LOGGER = Logger.new($stdout)
  LOGGER.level = OPTIONS[:debug]
  LOGGER.debug(OPTIONS)

  # Right now, `replay` means no tracing.
  # But we don't have a way of disabling post-processing
  folder = OPTIONS.include?(:replay) ? OPTIONS[:replay] || last_trace_saved : trace_and_on_node_processing(ARGV)
  if mpi_master?
    warn("THAPI: Trace location: #{folder}")
    gm_processing(folder) if OPTIONS[:analysis]
  end
end
