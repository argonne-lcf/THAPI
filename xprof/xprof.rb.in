require 'open3'
require 'fileutils'
require 'etc'
require 'optparse'
require 'pty'
require 'digest/md5'
require 'socket'
require 'logger'

#    _
#   |_ |\ | \  /
#   |_ | \|  \/
#
def env_fetches(*args, default: nil)
  @env_fetches ||= {}
  @env_fetches[args] ||= begin
    a = ENV.values_at(*args).compact
    a.empty? ? default : a.first
  end
end

#
#   \    / |_  o  _ |_  | o |_
#    \/\/  | | | (_ | | | | |_)
#
def ldconfig
  @ldconfig ||= begin
    LOGGER.debug { "Reading ldcondig" }
    ld_library_path = ENV['LD_LIBRARY_PATH'].gsub(':', ' ')
    stdout_str, _stderr_str, _status = Open3.capture3({ 'PATH' => "#{env_fetches('PATH')}:/sbin" },
                                                      "ldconfig -vNX #{ld_library_path}")
    stdout_str.split
  end
end

def exec(cmd)
  stdout_str, stderr_str, status = Open3.capture3(cmd)

  raise unless status.success?

  LOGGER.info { cmd }
  LOGGER.warn { stderr_str.strip } unless stderr_str.empty?
  LOGGER.debug { stdout_str.strip } unless stdout_str.empty?
  stdout_str
end

def whichlibs(pattern)
  header = ''
  ldconfig.filter_map do |l|
    # /opt/cray/libfabric/1.15.2.0/lib64:
    if l.end_with?(':')
      header = l.chop
      next
    end
    # libfabric.so.1 -> libfabric.so.1.18.2
    lib = l.split(' -> ', 1).first
    File.join(header, lib) if lib.match?(pattern)
  end
end

def whichlib64(pattern)
  whichlibs(pattern).lazy.filter do |lib|
    stdout_str, _stderr_str, _status = Open3.capture3("objdump -f #{lib}")
    # architecture: i386:x86-64, flags 0x00000150"
    stdout_str.match?(/architecture: \S+64/)
  end.first
end

#         _ ___    _
#   |\/| |_) |    |_ ._
#   |  | |  _|_   |_ | | \/
#
def in_mpi_env?
  env_fetches('PALS_RANKID', 'PMI_RANK') ? true : false
end

# Multiple iprof may run in parallel (for example using ctest),
# so use a random hex by default
require 'securerandom'
def mpi_job_id
  env_fetches('PALS_APID', 'PMI_JOBID', default: SecureRandom.hex)
end

def mpi_rank_id
  env_fetches('PALS_RANKID', 'PMI_RANK', default: 0).to_i
end

def mpi_local_size
  env_fetches('PALS_LOCAL_SIZE', 'PMI_LOCAL_SIZE', default: 1).to_i
end

def mpi_local_rank_id
  env_fetches('PALS_LOCAL_RANKID', 'PMI_LOCAL_RANK', default: 0).to_i
end

def mpi_local_master?
  mpi_local_rank_id == 0
end

def mpi_master?
  mpi_rank_id == 0
end

#    _
#   |_)  _. ._ ._ o  _  ._
#   |_) (_| |  |  | (/_ |
#
def count_file(folder)
  # \ls to avoid alias. Counter `.` and `..`, so remove them
  #  Will return -2 for a empty directory
  stdout_str, _stderr_str, _status = Open3.capture3("\\ls -afq #{folder}")
  stdout_str.lines.size - 2
end

FOLDER_JOBID = File.join('.thapi_lock', mpi_job_id)
# Put the user name, to avoid permission issue for people sharing nodes
SHARED_LOCAL_FILESYSTEM = File.join('/', 'dev', 'shm', Etc.getlogin, FOLDER_JOBID)
SHARED_GLOBAL_FILESYSTEM = File.join(env_fetches('HOME'), FOLDER_JOBID)

# Use a log distribution seem to be a good tradeoff
# between being nice to the FileSystem (not to many call)
# but not waiting to much
def busy_wait(&block)
  (2..).take_while { |i| block.call && sleep(Math.log(i)) }
end

# We know the local size, and the local id
#   Each process write a file, then we wait until all of them did it
#
# We cannot remove the local_path; this can lead to deadlock
#   One process finish and remove the file,
#   when another process is sleeping
# Hence, each local barrier should have a unique name
def local_barier(name)
  LOGGER.info { "Local_barier #{name}" }
  return unless in_mpi_env?

  folder = File.join(SHARED_LOCAL_FILESYSTEM, name)
  FileUtils.mkdir_p(File.join(folder, mpi_local_rank_id.to_s))
  busy_wait { count_file(folder) != mpi_local_size }
end

# We don't know the total number of ranks
#   -`PALS_NRANKS` look interesting but is not exported by default
#   - We may want to call `MPI_INIT()` our self, but that mean liking against MPI
#     making it a pain to install / configure THAPI
# We don't know either how many local masters we have
#   - MPI is not required to round-robin process between hosts
#   - We cannot use `PBS_NODEFILE`, as people can request N nodes, but launch <N process
#
# At the beginning of the script, each master will create a folder.
# Then, when hiting the barrier, each local master will remove his file
#  and wait until no more folders exist.
#
# This is racy! If one thread exit the barrier before any other enter it,
#    this thread will pass the barrier.
# We rely/We hope, that user will call an MPI_Barrier() && that we do enough work...
def init_global_barrier
  LOGGER.info { 'Init_global_barrier' }
  return unless in_mpi_env?

  f = File.join(SHARED_GLOBAL_FILESYSTEM, mpi_rank_id.to_s)
  FileUtils.mkdir_p(f)
  f
end

def global_barrier(f)
  LOGGER.info { 'Global_barrier' }
  return unless in_mpi_env?

  # Block until all the process have removed their sentinel,
  #    then master will clean the folder to be nice to the user
  #
  # Note that `mpi_master` can see the folder empty, and hence remove it
  #   at the same time as others threads sleep. They will wake up
  #   and see a deleted folder.
  # Fortunately `count_file` will return -2 for a non exciting folder
  #   hence we test for > 0 and not `!= 0`
  FileUtils.rm_rf(f)
  busy_wait { count_file(SHARED_GLOBAL_FILESYSTEM) > 0 }
  FileUtils.rm_rf(SHARED_GLOBAL_FILESYSTEM) if mpi_master?
end

#                        __
#   | _|_ _|_ ._   _    (_   _ _|_     ._
#   |_ |_  |_ | | (_|   __) (/_ |_ |_| |_)
#                  _|                  |

# We Cannot use "@ .. @" for libdir, bindir, and dataroodir
# as they will appear as bash "${exec_prefix}/lib"
# So for now we will rely on them having the default value,
#  (https://www.gnu.org/software/automake/manual/html_node/Standard-Directory-Variables.html)
# Will do some gsub + eval if required latter
EXEC_PREFIX = '@prefix@'
BINDIR = File.join(EXEC_PREFIX, 'bin')
LIBDIR = File.join(EXEC_PREFIX, 'lib')
PKGLIBDIR = File.join(LIBDIR, '@PACKAGE@')
PREFIX = '@prefix@'
DATAROOTDIR = File.join(PREFIX, 'share')
DATADIR = DATAROOTDIR

def append_env_tracers(h)
  h['LD_LIBRARY_PATH'] << File.join(PKGLIBDIR, 'ze')
  h['LTTNG_UST_ZE_LIBZE_LOADER'] << whichlib64('libze_loader.so') unless env_fetches('LTTNG_UST_ZE_LIBZE_LOADER')
  h['LD_PRELOAD'] << File.join(LIBDIR, 'libTracerZE.so')
end

def launch_usr_bin(env, cmd)
  LOGGER.info { "Launch_usr_bin #{cmd}" }
  bash_env = env.map do |k, v|
    v.prepend(env_fetches(k)) if env_fetches(k)
    [k, v.join(':')]
  end.to_h

  begin
    PTY.spawn(bash_env, *cmd) do |stdout, _stdin, _pid|
      stdout.each { |line| print line }
    rescue Errno::EIO
    end
  rescue PTY::ChildExited
    puts 'The child process exited!'
  end
end

def rename_to_human_readable_folder(lttng_output_root)
  # Multiple thapi can run concurrently.
  #   To avoid any race-condition, we try to MKDIR until we succeed

  # This is solving a consensus for the name,
  # make it simpler and only allow "rank" per job to do it
  raise unless mpi_master?

  date = Time.now.strftime('%Y-%m-%d--%Hh%Mm%Ss')
  path = (0..).each do |i|
    prefix =  i == 0 ? '' : "_#{i}"
    path = File.join(env_fetches('HOME'), 'lttng-traces', "thapi--#{date}#{prefix}")
    begin
      Dir.mkdir(path)
    rescue SystemCallError
      next
    else
      break path
    end
  end
  File.rename(lttng_output_root, path)
  path
end

def enable_events_ze(lttng_session_uuid, channel_name, tracing_mode: 'default', profiling: true)
  lttng_enable = "lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name}"
  case tracing_mode
  when 'minimal'
    exec("#{lttng_enable} $(cat #{datadir}/babeltrace_zeprofiling_apis.txt)")
    exec("#{lttng_enable} lttng_ust_ze_properties:device_timer")
  when 'full'
    exec("#{lttng_enable} lttng_ust_ze_build:log*")
    exec("#{lttng_enable} lttng_ust_ze_profiling:*") if profiling
    exec("#{lttng_enable} lttng_ust_ze_properties:*")
    exec("#{lttng_enable} lttng_ust_ze:*")
  when 'default'
    exec("#{lttng_enable} lttng_ust_ze_build:log*")
    exec("#{lttng_enable} lttng_ust_ze_profiling:*") if profiling
    # Wildcard using the * character are supported at the end of tracepoint names.
    #   https://lttng.org/man/1/lttng-enable-event/v2.8/#doc-_understanding_event_rule_conditions
    # Disable-event doesn't have wildcards
    # So we enable and disable on the same line
    ze_properties_disable = ['lttng_ust_ze_properties:memory_info_properties',
                             'lttng_ust_ze_properties:memory_info_range']
    exec("#{lttng_enable} lttng_ust_ze_properties:* -x #{ze_properties_disable.join(',')}")

    ze_disable_events = ['lttng_ust_ze:zeKernelSetArgumentValue*', 'lttng_ust_ze:ze*Get*Properties*',
                         'lttng_ust_ze:zeKernelGetName']
    ze_disable_query = ['lttng_ust_ze:*QueryStatus', 'lttng_ust_ze:*ProcAddrTable*']
    ze_disable_loader = ['lttng_ust_ze:*Loader*']
    ze_disable = ze_disable_events + ze_disable_query + ze_disable_loader
    exec("#{lttng_enable} lttng_ust_ze:* -x #{ze_disable.join(',')}")
  else
    raise("Tracing mode #{tracing_mode} not supported")
  end
end

def setup_lttng
  #  We launch one daemon per Node
  #  Hence the output need to be prefixed by hostname, so that each MPI local master will write into it
  lttng_output = File.join(env_fetches('HOME'), 'lttng-traces', "thapi--#{mpi_job_id}", Socket.gethostname)

  # Each session will have an UUID.
  # This will be used to set LLTNG_HOME to a uuid for this run
  lttng_session_uuid = Digest::MD5.hexdigest(lttng_output)

  # Because $HOME is shared, the sessiond daemon will not be able to get a lock.
  #   `Error: Could not get lock file $USER/.lttng/lttng-sessiond.lck, another instance is running.`
  #   We will use the blancket solution of setting LLTNG_HOME waiting for the more granular solution
  lttng_home = File.join('/', 'tmp', lttng_session_uuid)
  FileUtils.mkdir_p(lttng_home)
  ENV['LTTNG_HOME'] = lttng_home

  # The dawmon will be cleaned in the teardown
  exec('lttng-sessiond --daemonize')

  exec("lttng create #{lttng_session_uuid} -o #{lttng_output}")

  channel_name = 'blocking-channel'
  exec("lttng enable-channel --userspace --session=#{lttng_session_uuid} --blocking-timeout=inf #{channel_name}")
  exec("lttng add-context    --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} -t vpid -t vtid")

  enable_events_ze(lttng_session_uuid, channel_name, tracing_mode: OPTIONS[:tracing_mode],
                                                     profiling: OPTIONS[:profile])
  exec("lttng start #{lttng_session_uuid}")
  [lttng_output, lttng_home, lttng_session_uuid]
end

def teardown_lttng(lttng_session_uuid, lttng_home)
  exec("lttng destroy #{lttng_session_uuid}")
  # Need to kill the sessiond. It's safe, because each job have their own
  #   In theory opening this file is racy.
  #   It's possible that the sessiond spawn before writing in file
  #   In practice to so much work between the two. Should ne ok
  pid = File.read(File.join(lttng_home, '.lttng', 'lttng-sessiond.pid')).to_i
  LOGGER.info('Killing sessiond')
  Process.kill('SIGKILL', pid)
end

# Start and Stop lttng
def trace(usr_argv)
  # All master setup a futur global barrier
  global_barrier_h = init_global_barrier if mpi_local_master?

  # All rank Load Tracers and APILoaders Lib
  h = Hash.new { |h, k| h[k] = [] }
  append_env_tracers(h)

  # Local Master setup lttng, all other local rank wait for it completion
  lttng_output, lttng_home, lttng_session_uuid = setup_lttng if mpi_local_master?
  local_barier('waiting_for_lttng_setup')

  # Launch User Command
  launch_usr_bin(h, usr_argv)

  # We need to be sure that all the local rank finished
  # before local master stop the lttng session
  local_barier('waiting_for_application_ending')
  teardown_lttng(lttng_session_uuid, lttng_home) if mpi_local_master?

  # Ensure that all traces have been written so we can post-process them
  global_barrier(global_barrier_h) if mpi_local_master?

  # Master will move all the folders to a better folder
  rename_to_human_readable_folder(File.dirname(lttng_output)) if mpi_master?
end

#    _                                   _
#   |_)  _. |_   _  | _|_ ._ _.  _  _     )
#   |_) (_| |_) (/_ |  |_ | (_| (_ (/_   /_
#

def last_trace_saved
  Dir[File.join(env_fetches('HOME'), 'lttng-traces', 'thapi--*')].max_by { |f| File.mtime(f) }
end

# TODO: Use babeltrace_thapi as a LIB not a binary
def postprocess(folder, _options)
  LOGGER.info { "postprocess #{folder}" }
  return unless mpi_master?

  backend_level = OPTIONS[:backend]
  backends = backend_level.map { |name_level| name_level.split(':').first }.join(',')

  if OPTIONS.include?(:trace)
    opts = 'trace'
    opts << '--restrict '
    opts << '--context '
    opts << "--backend #{backends.join(',')} " unless backends.empty?

  elsif OPTIONS.include?(:timeline)
    opts = 'timeline'
    opts << "--backend #{backends.join(',')} " unless backends.empty?
  else
    opts = 'tally '
    opts << '--display_kernel_verbose ' if OPTIONS.include?(:'kernel-verbose')
    opts << '--display_metadata ' if OPTIONS.include?(:metadata)
    opts << "--display_name_max_size #{OPTIONS[:'max-name-size']} " if OPTIONS.include?(:'max-name-size')
    opts << '--display_mode json ' if OPTIONS.include?(:json)
    opts << "--backend #{backends.join(',')} " unless backends.empty?
    opts << "--backend_level #{backend_level.join(',')}" unless backend_level.empty?
    opts << '--display extended ' if OPTIONS.include?(:extended)
  end

  IO.popen("#{BINDIR}/babeltrace_thapi #{opts} -- #{folder}") do |stdout, _stdin, _pid|
    stdout.each { |line| print line }
  end
end

LOGGER = Logger.new($stdout)

#
#    _                       _    ___
#   |_) _. ._ _ o ._   _    /  |   |
#   |  (_| | _> | | | (_|   \_ |_ _|_
#                      _|
parser = OptionParser.new

# Tracing
parser.on('-m', '--tracing-mode=MODE', %w[minimal default full], 'Define the category of events traced')
parser.on('--[no-]profile', 'Device activities will not profiled')

# General Options
parser.on('-b', '--backend BACKEND', Array, "Select which and how backends' need to handled.",
          'Format: backend_name[:backend_level],...',
          'Default: omp:2,cl:1,ze:1,cuda:1,hip:1')

# Analysis
parser.on('-r', '--replay [PATH]', 'Replay traces for post-morten analysis')
parser.on('-t', '--trace', 'Pretty print the trace')
parser.on('-l', '--timeline', 'Dump a timeline of the trace.',
          "This will create a 'out.pftrace' file that can be opened in perfetto: https://ui.perfetto.dev/#!/viewer")
## Tally Specific Options
parser.on('-j', '--json', 'The tally will be dumped as json')
parser.on('-e', '--extended', 'The tally will be printed for each Hostname / Process / Thread / Device')
parser.on('-k', '--kernel-verbose',
          'The tally will report kernels execution time with SIMD width and global/local sizes')
parser.on('--max-name-size SIZE',
          'Maximum size allowed for kernels names (default 80)', '-1 mean no limit') { |size| size.to_i }

parser.on('--metadata', 'Display Trace Metadata')

# backend default are hardcoded inside the cpp for now
options = { tracing_mode: 'default', profile: true, backend: [] }

def print_help_and_exit(parser)
  puts(parser.help)
  puts(<<~EOF
                                                          __
    For complaints, praises, or bug reports please use: <(o )___
       https://github.com/argonne-lcf/THAPI              ( ._> /
       or send email to {apl,bvideau}@anl.gov             `---'
  EOF
      )
  exit 1
end

print_help_and_exit(parser) if ARGV.empty?

begin
  parser.parse!(into: options)
rescue OptionParser::InvalidOption, OptionParser::MissingArgument => e
  puts(e)
  print_help_and_exit(parser)
end

OPTIONS = options.freeze
LOGGER.debug(OPTIONS)

# Right now, `replay` mean no tracing.
# But we don't have a way of disabling post-pocessing
folder = if OPTIONS.include?(:replay)
           OPTIONS[:replay] || last_trace_saved
         else
           trace(ARGV)
         end

pp folder
postprocess(folder, {})
