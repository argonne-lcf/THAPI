require 'open3'
require 'fileutils'
require 'etc'
require 'optparse'
require 'pty'
require 'digest/md5'
require 'socket'

#
#   \    / |_  o  _ |_  | o |_
#    \/\/  | | | (_ | | | | |_)
#
def ldconfig
  @ldconfig ||= begin
    ld_library_path = ENV['LD_LIBRARY_PATH'].gsub(':', ' ')
    stdout_str, _stderr_str, _status = Open3.capture3({ 'PATH' => "#{ENV['PATH']}:/sbin" },
                                                      "ldconfig -vNX #{ld_library_path}")
    stdout_str.split
  end
end

def whichlibs(pattern)
  header = ''
  ldconfig.filter_map do |l|
    # /opt/cray/libfabric/1.15.2.0/lib64:
    if l.end_with?(':')
      header = l.chop
      next
    end
    # libfabric.so.1 -> libfabric.so.1.18.2
    lib = l.split(' -> ', 1).first
    File.join(header, lib) if lib.match?(pattern)
  end
end

def whichlib64(pattern)
  whichlibs(pattern).lazy.filter do |lib|
    stdout_str, _stderr_str, _status = Open3.capture3("objdump -f #{lib}")
    # architecture: i386:x86-64, flags 0x00000150"
    stdout_str.match?(/architecture: \S+64/)
  end.first
end

#         _ ___    _
#   |\/| |_) |    |_ ._
#   |  | |  _|_   |_ | | \/
#
def env_fetches(*args, default: nil)
  @env_fetches ||= {}
  @env_fetches[args] ||= begin
    a = ENV.values_at(*args).compact
    a.empty? ? default : a.first
  end
end

def in_mpi_env?
  env_fetches('PALS_RANKID', 'PMI_RANK') ? true : false
end

# Multiple iprof may run in parallel (for example using ctest),
# so use a random hex by default
require 'securerandom'
def mpi_job_id
  env_fetches('PALS_APID', 'PMI_JOBID', default: SecureRandom.hex)
end

def mpi_rank_id
  env_fetches('PALS_RANKID', 'PMI_RANK', default: 0).to_i
end

def mpi_local_size
  env_fetches('PALS_LOCAL_SIZE', 'PMI_LOCAL_SIZE', default: 1).to_i
end

def mpi_local_rank_id
  env_fetches('PALS_LOCAL_RANKID', 'PMI_LOCAL_RANK', default: 0).to_i
end

def mpi_local_master?
  mpi_local_rank_id == 0
end

def mpi_master?
  mpi_rank_id == 0
end

#    _
#   |_)  _. ._ ._ o  _  ._
#   |_) (_| |  |  | (/_ |
#
def count_file(folder)
  # \ls to avoid alias. Counter `.` and `..`, so remove them
  #  Will return -2 for a empty directory
  stdout_str, _stderr_str, _status = Open3.capture3("\\ls -afq #{folder}")
  stdout_str.lines.size - 2
end

FOLDER_JOBID = File.join('.thapi_lock', mpi_job_id)
# Put the user name, to avoid permission issue for people sharing nodes
SHARED_LOCAL_FILESYSTEM = File.join('/', 'dev', 'shm', Etc.getlogin, FOLDER_JOBID)
SHARED_GLOBAL_FILESYSTEM = File.join(ENV['HOME'], FOLDER_JOBID)

# Use a log distribution seem to be a good tradeoff
# between being nice to the FileSystem (not to many call)
# but not waiting to much
def busy_wait(&block)
  (2..).take_while { |i| block.call && sleep(Math.log(i)) }
end

# We know the local size, and the local id
#   Each process write a file, then we wait until all of them did it
#
# We cannot remove the local_path; this can lead to deadlock
#   One process finish and remove the file,
#   when another process is sleeping
# Hence, each local barrier should have a unique name
def local_barier(name)
  return unless in_mpi_env?

  folder = File.join(SHARED_LOCAL_FILESYSTEM, name)
  FileUtils.mkdir_p(File.join(folder, mpi_local_rank_id.to_s))
  busy_wait { count_file(folder) != mpi_local_size }
end

# We don't know the total number of ranks
#   -`PALS_NRANKS` look interesting but is not exported by default
#   - We may want to call `MPI_INIT()` our self, but that mean liking against MPI
#     making it a pain to install / configure THAPI
# We don't know either how many local masters we have
#   - MPI is not required to round-robin process between hosts
#   - We cannot use `PBS_NODEFILE`, as people can request N nodes, but launch <N process
#
# At the beginning of the script, each master will create a folder.
# Then, when hiting the barrier, each local master will remove his file
#  and wait until no more folders exist.
#
# This is racy! If one thread exit the barrier before any other enter it,
#    this thread will pass the barrier.
# We rely/We hope, that user will call an MPI_Barrier() && that we do enough work...
def init_global_barrier
  return unless in_mpi_env?

  # raise "Cannot be called by non master rank" unless mpi_local_master?
  f = File.join(SHARED_GLOBAL_FILESYSTEM, mpi_rank_id.to_s)
  FileUtils.mkdir_p(f)
  f
end

def global_barrier(f)
  return unless in_mpi_env?

  # raise "Cannot be called by non master rank" unless mpi_local_master?
  # Block until all the process have removed their sentinel,
  #    then master will clean the folder to be nice to the user
  #
  # Note that `mpi_master` can see the folder empty, and hence remove it
  #   at the same time as others threads sleep. They will wake up
  #   and see a deleted folder.
  # Fortunately `count_file` will return -2 for a non exciting folder
  #   hence we test for > 0 and not `!= 0`
  FileUtils.rm_rf(f)
  busy_wait { count_file(SHARED_GLOBAL_FILESYSTEM) > 0 }
  FileUtils.rm_rf(SHARED_GLOBAL_FILESYSTEM) if mpi_master?
end

#                        __
#   | _|_ _|_ ._   _    (_   _ _|_     ._
#   |_ |_  |_ | | (_|   __) (/_ |_ |_| |_)
#                  _|                  |

EXEC_PREFIX = @prefix@
LIBDIR = @libdir@
BINDIR = @bindir@
PKGLIBDIR = File.join(LIBDIR, @PACKAGE@)
PREFIX = @prefix@
DATAROOTDIR = @datarootdir@
DATADIR = @datadir@

def append_env_tracers(h)
  h['LD_LIBRARY_PATH'] << File.join(PKGLIBDIR, 'ze')
  h['LTTNG_UST_ZE_LIBZE_LOADER'] << whichlib64('libze_loader.so') unless env_fetches('LTTNG_UST_ZE_LIBZE_LOADER')
  h['LD_PRELOAD'] << File.join(LIBDIR, 'libTracerZE.so')
end

def launch_user_bin(env, cmd)
  bash_env = env.map do |k, v|
    v.prepend(env_fetches(k)) if env_fetches(k)
    [k, v.join(':')]
  end.to_h

  begin
    PTY.spawn(bash_env, *cmd) do |stdout, _stdin, _pid|
      stdout.each { |line| print line }
    rescue Errno::EIO
    end
  rescue PTY::ChildExited
    puts 'The child process exited!'
  end
end

def rename_to_human_readable_folder(lttng_output_root)
  # Multiple thapi can run concurrently.
  #   To avoid any race-condition, we try to MKDIR until we succeed
  
  # This is solving a consensus for the name,
  # make it simpler and only allow "rank" per job to do it
  raise unless mpi_master? 

  date = Time.now.strftime('%Y-%m-%d--%Hh%Mm%Ss')
  path = (0..).each do |i|
    prefix =  i == 0 ? '' : "_#{i}"
    path = File.join(ENV['HOME'], 'lttng-traces', "thapi--#{date}#{prefix}")
    begin
      Dir.mkdir(path)
    rescue SystemCallError
      next
    else
      break path
    end
  end
  File.rename(lttng_output_root, path)
  path
end

def enable_events_ze(lttng_session_uuid, channel_name)
  `lttng enable-event --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} lttng_ust_ze:*`
end

def setup_lttng
  #  We launch one daemon per Node
  #  Hence the output need to be prefixed by hostname, so that each MPI local master will write into it
  lttng_output = File.join(ENV['HOME'], 'lttng-traces', "thapi--#{mpi_job_id}", Socket.gethostname)

  # Each session will have an UUID.
  # This will be used to set LLTNG_HOME to a uuid for this run
  lttng_session_uuid = Digest::MD5.hexdigest(lttng_output)

  # Because $HOME is shared, the sessiond daemon will not be able to get a lock.
  #   `Error: Could not get lock file $USER/.lttng/lttng-sessiond.lck, another instance is running.`
  #   We will use the blancket solution of setting LLTNG_HOME waiting for the more granular solution
  lttng_home = File.join('/', 'tmp', lttng_session_uuid)
  FileUtils.mkdir_p(lttng_home)
  ENV['LTTNG_HOME'] = lttng_home

  # I don't know how to get the PID of `sessiond`, to then cleaning it up
  `lttng-sessiond --daemonize`

  `lttng create #{lttng_session_uuid} -o #{lttng_output}`

  channel_name = 'blocking-channel'
  `lttng enable-channel --userspace --session=#{lttng_session_uuid} --blocking-timeout=inf #{channel_name}`
  `lttng add-context    --userspace --session=#{lttng_session_uuid} --channel=#{channel_name} -t vpid -t vtid`

  enable_events_ze(lttng_session_uuid, channel_name)
  `lttng start #{lttng_session_uuid}`
  [lttng_output, lttng_home, lttng_session_uuid]
end

def teardown_lttng(lttng_session_uuid, lttng_home)
  `lttng destroy #{lttng_session_uuid}`
  # Need to kill the sessiond. It's safe, because each job have their own
  #   In theory opening this file is racy.
  #   It's possible that the sessiond spawn before writing in file
  #   In practice to so much work between the two. Should ne ok
  pid = File.read(File.join(lttng_home, '.lttng', 'lttng-sessiond.pid')).to_i
  Process.kill('SIGKILL', pid)
end

# Start and Stop lttng
def trace(usr_argv)
  # All master setup a futur global barrier
  global_barrier_h = init_global_barrier if mpi_local_master?

  # All rank Load Tracers and APILoaders Lib
  h = Hash.new { |h, k| h[k] = [] }
  append_env_tracers(h)

  # Local Master setup lttng, all other local rank wait for it completion
  lttng_output, lttng_home, lttng_session_uuid = setup_lttng if mpi_local_master?
  local_barier('waiting_for_lttng_setup')

  # Launch User Command
  launch_user_bin(h, usr_argv)

  # We need to be sure that all the local rank finished
  # before local master stop the lttng session
  local_barier('waiting_for_application_ending')
  teardown_lttng(lttng_session_uuid, lttng_home) if mpi_local_master?

  # Ensure that all traces have been written so we can post-process them
  global_barrier(global_barrier_h) if mpi_local_master?

  # Master will move all the folders to a better folder
  rename_to_human_readable_folder(File.dirname(lttng_output)) if mpi_master?
end

#    _                                   _
#   |_)  _. |_   _  | _|_ ._ _.  _  _     )
#   |_) (_| |_) (/_ |  |_ | (_| (_ (/_   /_
#

def last_trace_saved
  Dir[File.join(ENV['HOME'], 'lttng-traces', 'thapi--*')].max_by { |f| File.mtime(f) }
end

# TOOD: Use babeltrace_thapi as a LIB not a binary
def postprocess(folder, _options)
  return unless mpi_master?

  IO.popen("#{BINDIR}/babeltrace_thapi tally --backend cl,ze,cuda,omp,hip -- #{folder}") do |stdout, _stdin, _pid|
    stdout.each { |line| print line }
  end
end
#
#    _                       _    ___
#   |_) _. ._ _ o ._   _    /  |   |
#   |  (_| | _> | | | (_|   \_ |_ _|_
#                      _|
options = {}
OptionParser.new do |opts|
  opts.banner = 'Usage: example.rb [options]'

  opts.on('-h', '--help', 'Prints this help') do
    puts opts
    exit
  end

  opts.on('-r', '--replay [PATH]', 'Replay traces for post-morten analysis') do |path|
    options[:replay] = path
  end
end.parse!

# Right now, `replay` mean no tracing.
folder = if !options.include?(:replay)
           trace(ARGV)
         else
           # But we don't have a way of disabling post-pocessing
           options[:replay] || last_trace_saved
         end

pp folder
postprocess(folder, {})
